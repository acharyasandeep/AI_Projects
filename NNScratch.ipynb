{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "719b22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55baec",
   "metadata": {},
   "source": [
    "A Simple Python Class to  implement the Dense neural network layer\n",
    "\n",
    "Here, W and b are model parameters, and activation  is an element function (usually relu, but softmax for last layer)\n",
    "\n",
    "NaiveDense creates  two tensor variables, W  and  b, and exposes a __call__() method that applies the forward pass.\n",
    "\n",
    "```output = activation(dot(W, input)+b)```\n",
    "\n",
    "It's like tf.keras.layers.Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55c6319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "        \n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        \n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value  = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W)+self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b637d",
   "metadata": {},
   "source": [
    "A simple Sequential Class to chain the dense layers.\n",
    "\n",
    "It wraps a list of layers and exposes a __call__() method that simply calls the underlying layers on the inputs, in order. It also features a weights property to easily keep track of the layers’ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebcb7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        #Pushing all layers weights one by one in weights array.\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "            \n",
    "        return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2137eae",
   "metadata": {},
   "source": [
    "Using this NaiveDense class and this NaiveSequential class, we can create a mock Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb2f75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size= 28*28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b30b43",
   "metadata": {},
   "source": [
    "A batch Generator to iterate over the MNIST data in mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96d2eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images)/batch_size)\n",
    "        \n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cfe933",
   "metadata": {},
   "source": [
    "Difficult part is the \"training step\": updating the weights of the model after running it on one batch of data.\n",
    "\n",
    "1. Compute the predictions of the model for the images in the batch.\n",
    "2. Compute the loss value for these predictions, given the actual labels.\n",
    "3. Compute the gradient of the loss with regard to the model’s weights.\n",
    "4. Move the weights by a small amount in the direction opposite to the gradient.\n",
    "\n",
    "TensorFlow tf.GradientTape object is used to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42e0e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    \n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25ee42",
   "metadata": {},
   "source": [
    "Update weights by moving weights by \"a bit\" in a direction that will reduce the loss on this batch. Magnitude of the move is determined by the \"learning rate,\" typically a small quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8770de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-3\n",
    "\n",
    "# def update_weights(gradients, weights):\n",
    "#     for g, w  in zip(gradients, weights):\n",
    "#         w.assign_sub(g*learning_rate)\n",
    "        \n",
    "#In practice we don't implement update weight step like above by hand.\n",
    "#Instead we use an Optimizer instance from Keras, like:\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b1c79",
   "metadata": {},
   "source": [
    "An epoch of training consists of repeating the step for each batch in the training data, and the full training loop is simply the repetition of one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bd3f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        \n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if  batch_counter % 100== 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10b558",
   "metadata": {},
   "source": [
    "Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa5f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 5.13\n",
      "loss at batch 100: 2.22\n",
      "loss at batch 200: 2.19\n",
      "loss at batch 300: 2.10\n",
      "loss at batch 400: 2.22\n",
      "Epoch 1\n",
      "loss at batch 0: 1.90\n",
      "loss at batch 100: 1.87\n",
      "loss at batch 200: 1.82\n",
      "loss at batch 300: 1.72\n",
      "loss at batch 400: 1.84\n",
      "Epoch 2\n",
      "loss at batch 0: 1.57\n",
      "loss at batch 100: 1.57\n",
      "loss at batch 200: 1.50\n",
      "loss at batch 300: 1.43\n",
      "loss at batch 400: 1.52\n",
      "Epoch 3\n",
      "loss at batch 0: 1.32\n",
      "loss at batch 100: 1.33\n",
      "loss at batch 200: 1.24\n",
      "loss at batch 300: 1.22\n",
      "loss at batch 400: 1.29\n",
      "Epoch 4\n",
      "loss at batch 0: 1.12\n",
      "loss at batch 100: 1.15\n",
      "loss at batch 200: 1.04\n",
      "loss at batch 300: 1.06\n",
      "loss at batch 400: 1.12\n",
      "Epoch 5\n",
      "loss at batch 0: 0.98\n",
      "loss at batch 100: 1.01\n",
      "loss at batch 200: 0.90\n",
      "loss at batch 300: 0.94\n",
      "loss at batch 400: 1.00\n",
      "Epoch 6\n",
      "loss at batch 0: 0.87\n",
      "loss at batch 100: 0.91\n",
      "loss at batch 200: 0.80\n",
      "loss at batch 300: 0.85\n",
      "loss at batch 400: 0.91\n",
      "Epoch 7\n",
      "loss at batch 0: 0.79\n",
      "loss at batch 100: 0.82\n",
      "loss at batch 200: 0.72\n",
      "loss at batch 300: 0.78\n",
      "loss at batch 400: 0.85\n",
      "Epoch 8\n",
      "loss at batch 0: 0.72\n",
      "loss at batch 100: 0.76\n",
      "loss at batch 200: 0.66\n",
      "loss at batch 300: 0.72\n",
      "loss at batch 400: 0.79\n",
      "Epoch 9\n",
      "loss at batch 0: 0.67\n",
      "loss at batch 100: 0.70\n",
      "loss at batch 200: 0.61\n",
      "loss at batch 300: 0.68\n",
      "loss at batch 400: 0.75\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36371df",
   "metadata": {},
   "source": [
    "Evaluating the model by taking argmax of the predictions over the test images and comparing it to the expected labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dadfe23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759fc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
